# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import xgboost as xgb
import joblib

# Step 1: Load and Inspect the Data
df = pd.read_csv('credit_card_transactions.csv')  # Replace with your dataset path
print(df.head())
print("Dataset shape:", df.shape)

# Step 2: Basic Data Cleaning and Preprocessing
# Check for missing values
print("Missing values in the dataset:", df.isnull().sum())

# Drop rows with missing values (or you can fill them if needed)
df = df.dropna()

# Step 3: Feature Engineering
# In the case of the Kaggle dataset, most features are numerical except 'Class' (the target label)
# Scaling 'Amount' and 'Time' to bring them to a similar range
scaler = StandardScaler()
df[['Amount', 'Time']] = scaler.fit_transform(df[['Amount', 'Time']])

# Step 4: Separate Features and Labels
X = df.drop(columns=['Class'])  # Features: all columns except 'Class'
y = df['Class']  # Target: 'Class' (0 = non-fraud, 1 = fraud)

# Step 5: Split the Data into Training and Test Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 6: Train a Random Forest Model (You can also use XGBoost)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Hyperparameter tuning using GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}
grid_search_rf = GridSearchCV(rf_model, param_grid, cv=5)
grid_search_rf.fit(X_train, y_train)

# Step 7: Train an XGBoost Model
xgb_model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=42)
xgb_model.fit(X_train, y_train)

# Step 8: Evaluate both models

# Predict with Random Forest
y_pred_rf = grid_search_rf.best_estimator_.predict(X_test)
y_pred_proba_rf = grid_search_rf.best_estimator_.predict_proba(X_test)[:, 1]

# Predict with XGBoost
y_pred_xgb = xgb_model.predict(X_test)
y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]

# Classification report
print("Random Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))

print("XGBoost Classification Report:")
print(classification_report(y_test, y_pred_xgb))

# Confusion Matrix for Random Forest
cm_rf = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Random Forest Confusion Matrix')
plt.show()

# Confusion Matrix for XGBoost
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('XGBoost Confusion Matrix')
plt.show()

# ROC-AUC Curve for Random Forest
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)
plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='blue', label=f'Random Forest (AUC = {roc_auc_rf:.2f})')

# ROC-AUC Curve for XGBoost
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)
roc_auc_xgb = auc(fpr_xgb, tpr_xgb)
plt.plot(fpr_xgb, tpr_xgb, color='green', label=f'XGBoost (AUC = {roc_auc_xgb:.2f})')

plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

# Step 9: Save the Best Model for Deployment
# Choose the best model (let's assume XGBoost here, but you could also use Random Forest)
best_model = xgb_model  # or grid_search_rf.best_estimator_ for Random Forest
joblib.dump(best_model, 'credit_card_fraud_detection_model.pkl')
print("Model saved as 'credit_card_fraud_detection_model.pkl'")

# Step 10: Load the Model and Make Predictions on New Data (Example)
# Load the model for future predictions (e.g., in a production environment)
loaded_model = joblib.load('credit_card_fraud_detection_model.pkl')

# Simulating a new transaction (Replace with actual transaction data)
new_transaction = np.array([0.1, 0.5, 1000, -0.2, 1.0, ...])  # example feature values
new_transaction_scaled = scaler.transform(new_transaction.reshape(1, -1))  # Scale the features
fraud_prediction = loaded_model.predict(new_transaction_scaled)
print(f"Fraud Prediction (0 = Non-fraud, 1 = Fraud): {fraud_prediction}")
